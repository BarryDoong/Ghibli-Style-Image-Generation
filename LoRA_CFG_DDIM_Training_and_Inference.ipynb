{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f4ca40",
   "metadata": {},
   "source": [
    "# üé® LoRA Fine-Tuning with CFG + DDIM (Stable Diffusion v1.5)\n",
    "This notebook runs on Google Colab and supports training a LoRA model with Classifier-Free Guidance, followed by inference using all combinations of DDIM/CFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02150ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers torch accelerate peft bitsandbytes\n",
    "!pip install pytorch-fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dbdda",
   "metadata": {},
   "source": [
    "## üìÇ Upload your dataset\n",
    "Upload `captions.txt` and your training images in the `images/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb983b71",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Train LoRA with CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, PNDMScheduler\n",
    "from transformers import CLIPTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = 512\n",
    "ACCUMULATION_STEPS = 4\n",
    "PATIENCE = 3\n",
    "VALID_RATIO = 0.1\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"bias\": \"none\"\n",
    "}\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True\n",
    ")\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "def prepare_lora_model(model, target_modules):\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    lora_config = LoraConfig(**LORA_CONFIG, target_modules=target_modules)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = \"lora\" in name\n",
    "    return model\n",
    "\n",
    "unet = prepare_lora_model(pipe.unet.to(DEVICE), [\n",
    "    \"attn1.to_q\", \"attn1.to_k\", \"attn1.to_v\", \"attn1.to_out.0\",\n",
    "    \"attn2.to_q\", \"attn2.to_k\", \"attn2.to_v\", \"attn2.to_out.0\"\n",
    "]).train()\n",
    "text_encoder = prepare_lora_model(pipe.text_encoder.to(DEVICE), [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"\n",
    "]).train()\n",
    "vae = pipe.vae.to(DEVICE).eval()\n",
    "\n",
    "class GhibliDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_file):\n",
    "        self.image_dir = image_dir\n",
    "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "            self.data = [line.strip().split(\"|\") for line in f]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        fname, caption = self.data[idx]\n",
    "        image = Image.open(os.path.join(self.image_dir, fname)).convert(\"RGB\")\n",
    "        input_ids = tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77).input_ids.squeeze(0)\n",
    "        return self.transform(image), input_ids\n",
    "\n",
    "full_dataset = GhibliDataset(\"images\", \"captions.txt\")\n",
    "val_size = int(len(full_dataset) * VALID_RATIO)\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in unet.parameters() if p.requires_grad] + [p for p in text_encoder.parameters() if p.requires_grad],\n",
    "    lr=1e-4\n",
    ")\n",
    "scheduler_lr = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "scaler = GradScaler()\n",
    "\n",
    "def save_model(unet, text_encoder, vae, suffix):\n",
    "    unet.save_pretrained(f\"ghibli-unet-lora{suffix}\")\n",
    "    text_encoder.save_pretrained(f\"ghibli-text-lora{suffix}\")\n",
    "    vae.save_pretrained(f\"ghibli-vae{suffix}\")\n",
    "\n",
    "def evaluate_fid():\n",
    "    os.makedirs(\"val_generated\", exist_ok=True)\n",
    "    unet.eval()\n",
    "    text_encoder.eval()\n",
    "    for idx, (pixel_values, input_ids) in enumerate(val_loader):\n",
    "        pixel_values, input_ids = pixel_values.to(DEVICE), input_ids.to(DEVICE)\n",
    "        cond_emb = text_encoder(input_ids)[0]\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.size(0),), device=latents.device).long()\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "        with torch.no_grad(), autocast():\n",
    "            pred = unet(noisy_latents, timesteps, cond_emb).sample\n",
    "        pred_image = vae.decode(pred / 0.18215).sample\n",
    "        pred_image = (pred_image.clamp(-1, 1) + 1) / 2.0\n",
    "        save_path = f\"val_generated/gen_{idx}.png\"\n",
    "        transforms.ToPILImage()(pred_image[0].cpu()).save(save_path)\n",
    "    fid = fid_score.calculate_fid_given_paths([\"images\", \"val_generated\"], batch_size=1, device=DEVICE, dims=2048)\n",
    "    return fid\n",
    "\n",
    "best_loss, patience_counter = float('inf'), 0\n",
    "train_losses, fids = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    unet.train()\n",
    "    text_encoder.train()\n",
    "    vae.eval()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, (pixel_values, input_ids) in enumerate(tqdm(train_loader)):\n",
    "        pixel_values, input_ids = pixel_values.to(DEVICE), input_ids.to(DEVICE)\n",
    "\n",
    "        cond_emb = text_encoder(input_ids)[0]\n",
    "        uncond_ids = tokenizer([\"\"] * input_ids.size(0), return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77).input_ids.to(DEVICE)\n",
    "        uncond_emb = text_encoder(uncond_ids)[0]\n",
    "        encoder_hidden_states = torch.cat([uncond_emb, cond_emb])\n",
    "\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.size(0),), device=latents.device).long()\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        noisy_latents = torch.cat([noisy_latents, noisy_latents])\n",
    "        timesteps = torch.cat([timesteps, timesteps])\n",
    "        noise = torch.cat([noise, noise])\n",
    "\n",
    "        with autocast():\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "            guidance_scale = 7.5\n",
    "            noise_guided = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "            loss = nn.functional.mse_loss(noise_guided, noise[:input_ids.size(0)]) / ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item() * ACCUMULATION_STEPS\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    fid_score_val = evaluate_fid()\n",
    "    fids.append(fid_score_val)\n",
    "\n",
    "    print(f\"Avg Loss: {avg_loss:.4f}, FID: {fid_score_val:.2f}\")\n",
    "    scheduler_lr.step()\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        save_model(unet, text_encoder, vae, \"-best\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "save_model(unet, text_encoder, vae, \"\")\n",
    "\n",
    "# ÂÑ≤Â≠ò‰∏¶Áï´Âúñ\n",
    "np.save(\"train_losses.npy\", np.array(train_losses))\n",
    "np.save(\"fid_scores.npy\", np.array(fids))\n",
    "plt.plot(train_losses, label=\"Loss\")\n",
    "plt.plot(fids, label=\"FID\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss & FID\")\n",
    "plt.savefig(\"training_curves.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870b233",
   "metadata": {},
   "source": [
    "## üìâ Visualize training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "losses = np.load('train_losses.npy')\n",
    "fids = np.load('fid_scores.npy')\n",
    "plt.plot(losses, label='Loss')\n",
    "plt.plot(fids, label='FID')\n",
    "plt.legend()\n",
    "plt.title('Training Loss & FID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6b2b2",
   "metadata": {},
   "source": [
    "## üß™ Run Inference with DDIM/DDPM √ó CFG/NoCFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDIMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from peft import PeftModelPNDMScheduler\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt = \"a girl standing in a greenhouse, Studio Ghibli style\"\n",
    "\n",
    "# Load trained components\n",
    "unet = PeftModel.from_pretrained(UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\", torch_dtype=torch.float16), \"ghibli-unet-lora-best\").to(DEVICE).eval()\n",
    "text_encoder = PeftModel.from_pretrained(CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\", torch_dtype=torch.float16), \"ghibli-text-lora-best\").to(DEVICE).eval()\n",
    "vae = AutoencoderKL.from_pretrained(\"ghibli-vae-best\", torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def generate_image(use_ddim, guidance_scale, tag):\n",
    "    scheduler = DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\") if use_ddim else DDPMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionPipeline(\n",
    "        unet=unet,\n",
    "        text_encoder=text_encoder,\n",
    "        vae=vae,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=scheduler,\n",
    "        safety_checker=None,\n",
    "        feature_extractor=None,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    image = pipe(prompt, num_inference_steps=30, guidance_scale=guidance_scale).images[0]\n",
    "    image.save(f\"gen_{tag}.png\")\n",
    "    image.show()\n",
    "\n",
    "generate_image(use_ddim=False, guidance_scale=1.0, tag=\"ddpm_nocfg\")\n",
    "generate_image(use_ddim=False, guidance_scale=7.5, tag=\"ddpm_cfg\")\n",
    "generate_image(use_ddim=True, guidance_scale=1.0, tag=\"ddim_nocfg\")\n",
    "generate_image(use_ddim=True, guidance_scale=7.5, tag=\"ddim_cfg\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
